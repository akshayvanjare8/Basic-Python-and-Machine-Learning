{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworkforRegression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T-UNMHUxpVF"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "nbej1iBuzC06",
        "outputId": "f17095fc-fa42-4387-ed36-680c0bbe61e0"
      },
      "source": [
        "train = pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-114.31</td>\n",
              "      <td>34.19</td>\n",
              "      <td>15.0</td>\n",
              "      <td>5612.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>1015.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>1.4936</td>\n",
              "      <td>66900.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-114.47</td>\n",
              "      <td>34.40</td>\n",
              "      <td>19.0</td>\n",
              "      <td>7650.0</td>\n",
              "      <td>1901.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>463.0</td>\n",
              "      <td>1.8200</td>\n",
              "      <td>80100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-114.56</td>\n",
              "      <td>33.69</td>\n",
              "      <td>17.0</td>\n",
              "      <td>720.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>333.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1.6509</td>\n",
              "      <td>85700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.64</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1501.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>515.0</td>\n",
              "      <td>226.0</td>\n",
              "      <td>3.1917</td>\n",
              "      <td>73400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.57</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1454.0</td>\n",
              "      <td>326.0</td>\n",
              "      <td>624.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>1.9250</td>\n",
              "      <td>65500.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  ...  median_income  median_house_value\n",
              "0    -114.31     34.19  ...         1.4936             66900.0\n",
              "1    -114.47     34.40  ...         1.8200             80100.0\n",
              "2    -114.56     33.69  ...         1.6509             85700.0\n",
              "3    -114.57     33.64  ...         3.1917             73400.0\n",
              "4    -114.57     33.57  ...         1.9250             65500.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6_25Uz4zTpg"
      },
      "source": [
        "x_train = train.drop(columns = [\"median_house_value\"])\n",
        "y_train = train[\"median_house_value\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "RmXCyUVdzT2z",
        "outputId": "effdfc5f-37f5-4649-c1d9-484409e7fac7"
      },
      "source": [
        "test = pd.read_csv(\"/content/sample_data/california_housing_test.csv\")\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.05</td>\n",
              "      <td>37.37</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3885.0</td>\n",
              "      <td>661.0</td>\n",
              "      <td>1537.0</td>\n",
              "      <td>606.0</td>\n",
              "      <td>6.6085</td>\n",
              "      <td>344700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-118.30</td>\n",
              "      <td>34.26</td>\n",
              "      <td>43.0</td>\n",
              "      <td>1510.0</td>\n",
              "      <td>310.0</td>\n",
              "      <td>809.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>3.5990</td>\n",
              "      <td>176500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-117.81</td>\n",
              "      <td>33.78</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3589.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>1484.0</td>\n",
              "      <td>495.0</td>\n",
              "      <td>5.7934</td>\n",
              "      <td>270500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-118.36</td>\n",
              "      <td>33.82</td>\n",
              "      <td>28.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6.1359</td>\n",
              "      <td>330000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-119.67</td>\n",
              "      <td>36.33</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>850.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>2.9375</td>\n",
              "      <td>81700.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  ...  median_income  median_house_value\n",
              "0    -122.05     37.37  ...         6.6085            344700.0\n",
              "1    -118.30     34.26  ...         3.5990            176500.0\n",
              "2    -117.81     33.78  ...         5.7934            270500.0\n",
              "3    -118.36     33.82  ...         6.1359            330000.0\n",
              "4    -119.67     36.33  ...         2.9375             81700.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1Ie_cwPzT-s"
      },
      "source": [
        "x_test = test.drop(columns = [\"median_house_value\"])\n",
        "y_test = test[\"median_house_value\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcttGP7FzUGk",
        "outputId": "e70af685-89e5-46fa-9fd2-56a456c0a829"
      },
      "source": [
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17000, 8), (3000, 8), (17000,), (3000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SGGK2S6zUNB"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        " \n",
        "sc=StandardScaler()\n",
        " \n",
        "sc_x_train = sc.fit_transform(x_train)\n",
        "sc_x_test = sc.fit_transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqeD-9uZzUUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8cd4a7-b736-473c-923c-c82353794fd4"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        " \n",
        "model=Sequential()\n",
        " \n",
        "model.add(Dense(25, input_dim = 8, activation = \"relu\"))\n",
        "model.add(Dense(10, activation = \"relu\"))\n",
        "model.add(Dense(1, activation = \"linear\"))\n",
        "#model.compile(loss = \"mean_absolute_error\", optimizer = SGD(momentum = 0.6), metrics = [\"mse\"])\n",
        "model.compile(loss = \"mean_squared_logarithmic_error\", optimizer = SGD(momentum = 0.6), metrics = [\"mse\"])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 25)                225       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                260       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 496\n",
            "Trainable params: 496\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htcGVvuJzUbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed5b65fb-b245-47f0-8f17-823f3d0a88e7"
      },
      "source": [
        "help(SGD)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class SGD in module tensorflow.python.keras.optimizer_v2.gradient_descent:\n",
            "\n",
            "class SGD(tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2)\n",
            " |  SGD(learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD', **kwargs)\n",
            " |  \n",
            " |  Gradient descent (with momentum) optimizer.\n",
            " |  \n",
            " |  Update rule for parameter `w` with gradient `g` when `momentum` is 0:\n",
            " |  \n",
            " |  ```python\n",
            " |  w = w - learning_rate * g\n",
            " |  ```\n",
            " |  \n",
            " |  Update rule when `momentum` is larger than 0:\n",
            " |  \n",
            " |  ```python\n",
            " |  velocity = momentum * velocity - learning_rate * g\n",
            " |  w = w + velocity\n",
            " |  ```\n",
            " |  \n",
            " |  When `nesterov=True`, this rule becomes:\n",
            " |  \n",
            " |  ```python\n",
            " |  velocity = momentum * velocity - learning_rate * g\n",
            " |  w = w + momentum * velocity - learning_rate * g\n",
            " |  ```\n",
            " |  \n",
            " |  Args:\n",
            " |    learning_rate: A `Tensor`, floating point value, or a schedule that is a\n",
            " |      `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n",
            " |      that takes no arguments and returns the actual value to use. The\n",
            " |      learning rate. Defaults to 0.01.\n",
            " |    momentum: float hyperparameter >= 0 that accelerates gradient descent\n",
            " |      in the relevant\n",
            " |      direction and dampens oscillations. Defaults to 0, i.e., vanilla gradient\n",
            " |      descent.\n",
            " |    nesterov: boolean. Whether to apply Nesterov momentum.\n",
            " |      Defaults to `False`.\n",
            " |    name: Optional name prefix for the operations created when applying\n",
            " |      gradients.  Defaults to `\"SGD\"`.\n",
            " |    **kwargs: Keyword arguments. Allowed to be one of\n",
            " |      `\"clipnorm\"` or `\"clipvalue\"`.\n",
            " |      `\"clipnorm\"` (float) clips gradients by norm; `\"clipvalue\"` (float) clips\n",
            " |      gradients by value.\n",
            " |  \n",
            " |  Usage:\n",
            " |  \n",
            " |  >>> opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
            " |  >>> var = tf.Variable(1.0)\n",
            " |  >>> loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1\n",
            " |  >>> step_count = opt.minimize(loss, [var]).numpy()\n",
            " |  >>> # Step is `- learning_rate * grad`\n",
            " |  >>> var.numpy()\n",
            " |  0.9\n",
            " |  \n",
            " |  >>> opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
            " |  >>> var = tf.Variable(1.0)\n",
            " |  >>> val0 = var.value()\n",
            " |  >>> loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1\n",
            " |  >>> # First step is `- learning_rate * grad`\n",
            " |  >>> step_count = opt.minimize(loss, [var]).numpy()\n",
            " |  >>> val1 = var.value()\n",
            " |  >>> (val0 - val1).numpy()\n",
            " |  0.1\n",
            " |  >>> # On later steps, step-size increases because of momentum\n",
            " |  >>> step_count = opt.minimize(loss, [var]).numpy()\n",
            " |  >>> val2 = var.value()\n",
            " |  >>> (val1 - val2).numpy()\n",
            " |  0.18\n",
            " |  \n",
            " |  Reference:\n",
            " |      - For `nesterov=True`, See [Sutskever et al., 2013](\n",
            " |        http://jmlr.org/proceedings/papers/v28/sutskever13.pdf).\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      SGD\n",
            " |      tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2\n",
            " |      tensorflow.python.training.tracking.base.Trackable\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD', **kwargs)\n",
            " |      Create a new Optimizer.\n",
            " |      \n",
            " |      This must be called by the constructors of subclasses.\n",
            " |      Note that Optimizer instances should not bind to a single graph,\n",
            " |      and so shouldn't keep Tensors as member variables. Generally\n",
            " |      you should be able to use the _set_hyper()/state.get_hyper()\n",
            " |      facility instead.\n",
            " |      \n",
            " |      This class is stateful and thread-compatible.\n",
            " |      \n",
            " |      Example of custom gradient transformations:\n",
            " |      \n",
            " |      ```python\n",
            " |      def my_gradient_transformer(grads_and_vars):\n",
            " |        # Simple example, double the gradients.\n",
            " |        return [(2. * g, v) for g, v in grads_and_vars]\n",
            " |      \n",
            " |      optimizer = tf.keras.optimizers.SGD(\n",
            " |          1e-3, gradient_transformers=[my_gradient_transformer])\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        name: String. The name to use for momentum accumulator weights created\n",
            " |          by the optimizer.\n",
            " |        gradient_aggregator: The function to use to aggregate gradients across\n",
            " |          devices (when using `tf.distribute.Strategy`). If `None`, defaults to\n",
            " |          summing the gradients across devices. The function should accept and\n",
            " |          return a list of `(gradient, variable)` tuples.\n",
            " |        gradient_transformers: Optional. List of functions to use to transform\n",
            " |          gradients before applying updates to Variables. The functions are\n",
            " |          applied after `gradient_aggregator`. The functions should accept and\n",
            " |          return a list of `(gradient, variable)` tuples.\n",
            " |        **kwargs: keyword arguments. Allowed arguments are `clipvalue`,\n",
            " |          `clipnorm`, `global_clipnorm`.\n",
            " |          If `clipvalue` (float) is set, the gradient of each weight\n",
            " |          is clipped to be no higher than this value.\n",
            " |          If `clipnorm` (float) is set, the gradient of each weight\n",
            " |          is individually clipped so that its norm is no higher than this value.\n",
            " |          If `global_clipnorm` (float) is set the gradient of all weights is\n",
            " |          clipped so that their global norm is no higher than this value.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: in case of any invalid argument.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the optimizer.\n",
            " |      \n",
            " |      An optimizer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of an optimizer.\n",
            " |      The same optimizer can be reinstantiated later\n",
            " |      (without any saved state) from this configuration.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2:\n",
            " |  \n",
            " |  __dir__(self)\n",
            " |      Default dir() implementation.\n",
            " |  \n",
            " |  __getattribute__(self, name)\n",
            " |      Overridden to support hyperparameter access.\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Override setattr to support dynamic hyperparameter setting.\n",
            " |  \n",
            " |  add_slot(self, var, slot_name, initializer='zeros')\n",
            " |      Add a new slot variable for `var`.\n",
            " |  \n",
            " |  add_weight(self, name, shape, dtype=None, initializer='zeros', trainable=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>)\n",
            " |  \n",
            " |  apply_gradients(self, grads_and_vars, name=None, experimental_aggregate_gradients=True)\n",
            " |      Apply gradients to variables.\n",
            " |      \n",
            " |      This is the second part of `minimize()`. It returns an `Operation` that\n",
            " |      applies gradients.\n",
            " |      \n",
            " |      The method sums gradients from all replicas in the presence of\n",
            " |      `tf.distribute.Strategy` by default. You can aggregate gradients yourself by\n",
            " |      passing `experimental_aggregate_gradients=False`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      grads = tape.gradient(loss, vars)\n",
            " |      grads = tf.distribute.get_replica_context().all_reduce('sum', grads)\n",
            " |      # Processing aggregated gradients.\n",
            " |      optimizer.apply_gradients(zip(grads, vars),\n",
            " |          experimental_aggregate_gradients=False)\n",
            " |      \n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        grads_and_vars: List of (gradient, variable) pairs.\n",
            " |        name: Optional name for the returned operation. Default to the name passed\n",
            " |          to the `Optimizer` constructor.\n",
            " |        experimental_aggregate_gradients: Whether to sum gradients from different\n",
            " |          replicas in the presense of `tf.distribute.Strategy`. If False, it's\n",
            " |          user responsibility to aggregate the gradients. Default to True.\n",
            " |      \n",
            " |      Returns:\n",
            " |        An `Operation` that applies the specified gradients. The `iterations`\n",
            " |        will be automatically increased by 1.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If `grads_and_vars` is malformed.\n",
            " |        ValueError: If none of the variables have gradients.\n",
            " |        RuntimeError: If called in a cross-replica context.\n",
            " |  \n",
            " |  get_gradients(self, loss, params)\n",
            " |      Returns gradients of `loss` with respect to `params`.\n",
            " |      \n",
            " |      Should be used only in legacy v1 graph mode.\n",
            " |      \n",
            " |      Arguments:\n",
            " |        loss: Loss tensor.\n",
            " |        params: List of variables.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of gradient tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: In case any gradient cannot be computed (e.g. if gradient\n",
            " |          function not implemented).\n",
            " |  \n",
            " |  get_slot(self, var, slot_name)\n",
            " |  \n",
            " |  get_slot_names(self)\n",
            " |      A list of names for this optimizer's slots.\n",
            " |  \n",
            " |  get_updates(self, loss, params)\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Returns the current weights of the optimizer.\n",
            " |      \n",
            " |      The weights of an optimizer are its state (ie, variables).\n",
            " |      This function returns the weight values associated with this\n",
            " |      optimizer as a list of Numpy arrays. The first value is always the\n",
            " |      iterations count of the optimizer, followed by the optimizer's state\n",
            " |      variables in the order they were created. The returned list can in turn\n",
            " |      be used to load state into similarly parameterized optimizers.\n",
            " |      \n",
            " |      For example, the RMSprop optimizer for this simple model returns a list of\n",
            " |      three values-- the iteration count, followed by the root-mean-square value\n",
            " |      of the kernel and bias of the single Dense layer:\n",
            " |      \n",
            " |      >>> opt = tf.keras.optimizers.RMSprop()\n",
            " |      >>> m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
            " |      >>> m.compile(opt, loss='mse')\n",
            " |      >>> data = np.arange(100).reshape(5, 20)\n",
            " |      >>> labels = np.zeros(5)\n",
            " |      >>> print('Training'); results = m.fit(data, labels)\n",
            " |      Training ...\n",
            " |      >>> len(opt.get_weights())\n",
            " |      3\n",
            " |      \n",
            " |      Returns:\n",
            " |          Weights values as a list of numpy arrays.\n",
            " |  \n",
            " |  minimize(self, loss, var_list, grad_loss=None, name=None, tape=None)\n",
            " |      Minimize `loss` by updating `var_list`.\n",
            " |      \n",
            " |      This method simply computes gradient using `tf.GradientTape` and calls\n",
            " |      `apply_gradients()`. If you want to process the gradient before applying\n",
            " |      then call `tf.GradientTape` and `apply_gradients()` explicitly instead\n",
            " |      of using this function.\n",
            " |      \n",
            " |      Args:\n",
            " |        loss: `Tensor` or callable. If a callable, `loss` should take no arguments\n",
            " |          and return the value to minimize. If a `Tensor`, the `tape` argument\n",
            " |          must be passed.\n",
            " |        var_list: list or tuple of `Variable` objects to update to minimize\n",
            " |          `loss`, or a callable returning the list or tuple of `Variable` objects.\n",
            " |          Use callable when the variable list would otherwise be incomplete before\n",
            " |          `minimize` since the variables are created at the first time `loss` is\n",
            " |          called.\n",
            " |        grad_loss: (Optional). A `Tensor` holding the gradient computed for\n",
            " |          `loss`.\n",
            " |        name: (Optional) str. Name for the returned operation.\n",
            " |        tape: (Optional) `tf.GradientTape`. If `loss` is provided as a `Tensor`,\n",
            " |          the tape that computed the `loss` must be provided.\n",
            " |      \n",
            " |      Returns:\n",
            " |        An `Operation` that updates the variables in `var_list`. The `iterations`\n",
            " |        will be automatically increased by 1.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: If some of the variables are not `Variable` objects.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Set the weights of the optimizer.\n",
            " |      \n",
            " |      The weights of an optimizer are its state (ie, variables).\n",
            " |      This function takes the weight values associated with this\n",
            " |      optimizer as a list of Numpy arrays. The first value is always the\n",
            " |      iterations count of the optimizer, followed by the optimizer's state\n",
            " |      variables in the order they are created. The passed values are used to set\n",
            " |      the new state of the optimizer.\n",
            " |      \n",
            " |      For example, the RMSprop optimizer for this simple model takes a list of\n",
            " |      three values-- the iteration count, followed by the root-mean-square value\n",
            " |      of the kernel and bias of the single Dense layer:\n",
            " |      \n",
            " |      >>> opt = tf.keras.optimizers.RMSprop()\n",
            " |      >>> m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
            " |      >>> m.compile(opt, loss='mse')\n",
            " |      >>> data = np.arange(100).reshape(5, 20)\n",
            " |      >>> labels = np.zeros(5)\n",
            " |      >>> print('Training'); results = m.fit(data, labels)\n",
            " |      Training ...\n",
            " |      >>> new_weights = [np.array(10), np.ones([20, 10]), np.zeros([10])]\n",
            " |      >>> opt.set_weights(new_weights)\n",
            " |      >>> opt.iterations\n",
            " |      <tf.Variable 'RMSprop/iter:0' shape=() dtype=int64, numpy=10>\n",
            " |      \n",
            " |      Arguments:\n",
            " |          weights: weight values as a list of numpy arrays.\n",
            " |  \n",
            " |  variables(self)\n",
            " |      Returns variables of this Optimizer based on the order created.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2:\n",
            " |  \n",
            " |  from_config(config, custom_objects=None) from abc.ABCMeta\n",
            " |      Creates an optimizer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same optimizer from the config\n",
            " |      dictionary.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          config: A Python dictionary, typically the output of get_config.\n",
            " |          custom_objects: A Python dictionary mapping names to additional Python\n",
            " |            objects used to create this optimizer, such as a function used for a\n",
            " |            hyperparameter.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An optimizer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.keras.optimizer_v2.optimizer_v2.OptimizerV2:\n",
            " |  \n",
            " |  clipnorm\n",
            " |      `float` or `None`. If set, clips gradients to a maximum norm.\n",
            " |  \n",
            " |  clipvalue\n",
            " |      `float` or `None`. If set, clips gradients to a maximum value.\n",
            " |  \n",
            " |  global_clipnorm\n",
            " |      `float` or `None`. If set, clips gradients to a maximum norm.\n",
            " |  \n",
            " |  iterations\n",
            " |      Variable. The number of training steps this Optimizer has run.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns variables of this Optimizer based on the order created.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcQbvSnGzUiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a0e55e-e19b-47cc-daa1-351ed8708fb7"
      },
      "source": [
        "history = model.fit(sc_x_train, y_train, epochs = 100, batch_size = 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "340/340 [==============================] - 1s 1ms/step - loss: 52.9509 - mse: 56381600452.6921\n",
            "Epoch 2/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 17.2834 - mse: 54901738414.9208\n",
            "Epoch 3/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 12.0203 - mse: 54193314806.9912\n",
            "Epoch 4/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 9.2641 - mse: 52182354394.4633\n",
            "Epoch 5/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 7.6392 - mse: 51829555091.8944\n",
            "Epoch 6/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 6.4406 - mse: 50271232612.5982\n",
            "Epoch 7/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 5.6128 - mse: 49842681681.8299\n",
            "Epoch 8/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 4.9032 - mse: 47970010024.9150\n",
            "Epoch 9/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 4.3543 - mse: 46973662397.1848\n",
            "Epoch 10/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 3.9281 - mse: 46111762750.3109\n",
            "Epoch 11/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 3.5261 - mse: 44145131111.6012\n",
            "Epoch 12/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 3.2138 - mse: 42952530995.0499\n",
            "Epoch 13/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 2.9698 - mse: 42924995208.6334\n",
            "Epoch 14/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 2.7314 - mse: 41878447965.8416\n",
            "Epoch 15/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 2.5483 - mse: 41457598115.6598\n",
            "Epoch 16/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 2.3513 - mse: 40518690389.5836\n",
            "Epoch 17/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 2.1664 - mse: 38487159108.3167\n",
            "Epoch 18/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 2.0588 - mse: 38904873656.6804\n",
            "Epoch 19/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 1.9333 - mse: 38271790335.2493\n",
            "Epoch 20/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 1.8192 - mse: 37443444015.2962\n",
            "Epoch 21/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 1.7055 - mse: 36274546453.7713\n",
            "Epoch 22/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 1.6132 - mse: 35743019704.6804\n",
            "Epoch 23/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 1.5085 - mse: 34120195099.0264\n",
            "Epoch 24/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 1.4322 - mse: 33783754595.8475\n",
            "Epoch 25/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 1.3709 - mse: 33310324669.9355\n",
            "Epoch 26/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 1.2903 - mse: 32284142018.4399\n",
            "Epoch 27/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 1.2544 - mse: 32866844107.4487\n",
            "Epoch 28/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 1.1776 - mse: 31312602030.9208\n",
            "Epoch 29/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 1.1458 - mse: 31873223088.4223\n",
            "Epoch 30/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 1.0908 - mse: 30943390440.7273\n",
            "Epoch 31/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 1.0426 - mse: 30115548895.7185\n",
            "Epoch 32/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.9973 - mse: 29510655873.8768\n",
            "Epoch 33/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.9635 - mse: 29359003834.1818\n",
            "Epoch 34/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.9206 - mse: 28537503509.7713\n",
            "Epoch 35/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.8810 - mse: 27778614671.3900\n",
            "Epoch 36/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.8677 - mse: 28095587162.8387\n",
            "Epoch 37/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.8303 - mse: 27433596261.3490\n",
            "Epoch 38/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.8072 - mse: 27112810309.8182\n",
            "Epoch 39/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.7797 - mse: 26811215331.4721\n",
            "Epoch 40/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.7393 - mse: 25691726929.0792\n",
            "Epoch 41/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.7304 - mse: 26037300797.5601\n",
            "Epoch 42/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.6924 - mse: 25117642439.6950\n",
            "Epoch 43/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.6632 - mse: 24014222873.5249\n",
            "Epoch 44/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.6599 - mse: 24805250135.0850\n",
            "Epoch 45/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.6379 - mse: 23820522138.6510\n",
            "Epoch 46/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.6257 - mse: 24096083223.2727\n",
            "Epoch 47/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.6025 - mse: 23213757533.0909\n",
            "Epoch 48/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.5821 - mse: 22854828212.1760\n",
            "Epoch 49/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.5797 - mse: 23169011303.6012\n",
            "Epoch 50/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.5578 - mse: 22388504810.2287\n",
            "Epoch 51/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.5475 - mse: 22182326884.5982\n",
            "Epoch 52/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.5267 - mse: 21654855854.1701\n",
            "Epoch 53/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.5201 - mse: 21733801263.2962\n",
            "Epoch 54/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.5040 - mse: 21183903275.5425\n",
            "Epoch 55/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.5011 - mse: 21371157549.0440\n",
            "Epoch 56/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.4956 - mse: 21466158034.9560\n",
            "Epoch 57/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.4700 - mse: 20133168725.5836\n",
            "Epoch 58/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.4693 - mse: 20558283118.3578\n",
            "Epoch 59/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.4533 - mse: 19831415738.9326\n",
            "Epoch 60/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.4449 - mse: 19764323204.8798\n",
            "Epoch 61/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.4350 - mse: 19456111985.3607\n",
            "Epoch 62/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.4324 - mse: 19450387843.3783\n",
            "Epoch 63/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.4225 - mse: 19269902657.3138\n",
            "Epoch 64/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.4164 - mse: 19032953234.3930\n",
            "Epoch 65/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.4083 - mse: 18791373574.7566\n",
            "Epoch 66/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3991 - mse: 18473483948.6686\n",
            "Epoch 67/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3981 - mse: 18610230575.2962\n",
            "Epoch 68/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3903 - mse: 18372788620.3871\n",
            "Epoch 69/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3844 - mse: 18347302359.4604\n",
            "Epoch 70/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3779 - mse: 17878022150.0059\n",
            "Epoch 71/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3714 - mse: 17590788795.6833\n",
            "Epoch 72/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3689 - mse: 17797880561.7361\n",
            "Epoch 73/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3563 - mse: 17031234596.0352\n",
            "Epoch 74/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3648 - mse: 17718974995.5191\n",
            "Epoch 75/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3456 - mse: 16849682188.7625\n",
            "Epoch 76/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3459 - mse: 16735374387.0499\n",
            "Epoch 77/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3376 - mse: 16308981258.5103\n",
            "Epoch 78/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3337 - mse: 16396418930.8622\n",
            "Epoch 79/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3307 - mse: 16311353563.2141\n",
            "Epoch 80/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3323 - mse: 16429360311.1789\n",
            "Epoch 81/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3284 - mse: 16513617274.3695\n",
            "Epoch 82/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3210 - mse: 15947955034.8387\n",
            "Epoch 83/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3174 - mse: 15703627806.0293\n",
            "Epoch 84/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3119 - mse: 15586473470.4985\n",
            "Epoch 85/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3108 - mse: 15501626482.1114\n",
            "Epoch 86/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3053 - mse: 15443891341.1378\n",
            "Epoch 87/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3061 - mse: 15367704200.6334\n",
            "Epoch 88/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.3015 - mse: 15269800488.5396\n",
            "Epoch 89/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.2991 - mse: 15254508805.2551\n",
            "Epoch 90/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.2979 - mse: 15061053358.9208\n",
            "Epoch 91/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.2907 - mse: 14608024161.5953\n",
            "Epoch 92/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.2866 - mse: 14680628548.3167\n",
            "Epoch 93/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.2855 - mse: 14443638910.1232\n",
            "Epoch 94/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.2792 - mse: 14055580813.1378\n",
            "Epoch 95/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.2814 - mse: 14371409076.1760\n",
            "Epoch 96/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.2780 - mse: 14145630403.1906\n",
            "Epoch 97/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.2780 - mse: 14121863795.6129\n",
            "Epoch 98/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.2743 - mse: 14093665742.4516\n",
            "Epoch 99/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.2766 - mse: 14283271402.2287\n",
            "Epoch 100/100\n",
            "340/340 [==============================] - 0s 1ms/step - loss: 0.2720 - mse: 14017948005.3490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQS5I3x8zUoi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "043844c7-968f-4be1-ffe0-3fb0d5d124c7"
      },
      "source": [
        "plt.plot(history.history[\"loss\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJPAdxHdzUu9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "4026e71a-98df-4ff7-b214-5836338205db"
      },
      "source": [
        "plt.plot(history.history[\"mse\"])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqfMBIOizU1a"
      },
      "source": [
        "y_pred = model.predict(sc_x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwNyIdkvzU-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf4dbea-2b05-40e4-e4c7-63bf987cb4c4"
      },
      "source": [
        "y_pred.flatten()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([172253.38, 149794.17, 172426.61, ..., 114042.66, 151968.03,\n",
              "       217115.62], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiUOxD1QzVEk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "4fbb0c66-deb5-4283-b215-9ca032d4ae85"
      },
      "source": [
        "pd.DataFrame({\"Actual Value\" : y_test, \"Predicted\" : y_pred.flatten()})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Actual Value</th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>344700.0</td>\n",
              "      <td>172253.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>176500.0</td>\n",
              "      <td>149794.171875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>270500.0</td>\n",
              "      <td>172426.609375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>330000.0</td>\n",
              "      <td>185599.906250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>81700.0</td>\n",
              "      <td>124601.320312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>225000.0</td>\n",
              "      <td>108954.273438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>237200.0</td>\n",
              "      <td>138296.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>62000.0</td>\n",
              "      <td>114042.656250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>162500.0</td>\n",
              "      <td>151968.031250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>500001.0</td>\n",
              "      <td>217115.625000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3000 rows  2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Actual Value      Predicted\n",
              "0         344700.0  172253.375000\n",
              "1         176500.0  149794.171875\n",
              "2         270500.0  172426.609375\n",
              "3         330000.0  185599.906250\n",
              "4          81700.0  124601.320312\n",
              "...            ...            ...\n",
              "2995      225000.0  108954.273438\n",
              "2996      237200.0  138296.890625\n",
              "2997       62000.0  114042.656250\n",
              "2998      162500.0  151968.031250\n",
              "2999      500001.0  217115.625000\n",
              "\n",
              "[3000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}